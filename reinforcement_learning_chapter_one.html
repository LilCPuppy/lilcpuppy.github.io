<!DOCTYPE html> 
<html lang="en-US" xml:lang="en-US" > 
<head>





<title></title> 
<meta  charset="utf-8" /> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" /> 
<link rel="stylesheet" type="text/css" href="reinforcement_learning_chapter_one.css" /> 
<meta name="src" content="reinforcement_learning_chapter_one.tex"> 
</head><body 
>





<!--l. 34--><p class="noindent" > 


 


<div class="center" 
>


<!--l. 37--><p class="noindent" >











<!--l. 40--><p class="noindent" ><span id="textcolor1"><span 
class="cmbx-12x-x-207">Chapter</span>


<span 
class="cmbx-12x-x-207">One</span></span>





</div>


<!--l. 45--><p class="noindent" > 


 





<h3 class="likesectionHead"><a 
 id="x1-1000"></a><span id="textcolor2">Overview</span></h3>


<div class="newtheorem">





<!--l. 51--><p class="noindent" ><span class="head">


<a 
 id="x1-1001r1"></a>








<span 
class="cmbx-10x-x-109">Definition 1.</span>


</span><span 
class="cmti-10x-x-109">Reinforcement</span>


<span 
class="cmti-10x-x-109">Learning</span>


defines


a


process


of


how


to


map


situations


to


actions


such


that


a


numerical


reward


signal


is


maximized.


</div>





<!--l. 55--><p class="noindent" >One


often


thinks


of


supervised


learning


when


considering


ML,


and


reinforcement


learning


differs


in


a


distinct


way:


RL


problems


are


interactive.


The


environment


can


change


around


the


agent,


and


the


agent


could


possibly


change


the


environment.


<!--l. 57--><p class="noindent" >There


are


two


distinguishing


characteristics


associated


with


reinforcement


learning


problems:





     <ul class="itemize1">





     <li class="itemize">


     Trial


     and


     error


     search,


     and


     </li>





     <li class="itemize">


     Delayed


     reward</li></ul>


<!--l. 64--><p class="noindent" >What


makes


these


distinguishing?


The


<span 
class="cmti-10x-x-109">trial</span>


<span 
class="cmti-10x-x-109">and</span>


<span 
class="cmti-10x-x-109">error</span>


ensure


that


the


agent


(or


learner)


will


try


new


things.


As


a


result,


we


mitigate


the


possibility


of


the


agent


finding


some


local


maximum


with


respect


to


the


reward


signal,


and


will


continue


to


explore


the


state


space.


The


<span 
class="cmti-10x-x-109">delayed</span>


<span 
class="cmti-10x-x-109">reward</span>


means


that


the


agent


must


think


about


the


future,


and


the


possible


long


term


effects


of


the


current


action.


<!--l. 66--><p class="noindent" >Another


facet


of


machine


learning


is


the


trade-off


between


exploration


and


exploitation?


Should


the


agent


take


advantage


of


moves/states


that


it


already


knows?


Or


should


it


consider


exploring


a


new


state?


This


is


a


dilemma


that


does


not


even


arise


in


conventional


supervised


learning,


and


has


been


studied


extensively


by


mathematicians


for


decades.


<!--l. 68--><p class="noindent" >Now,


RL


problems


are


<span 
class="cmti-10x-x-109">goal</span>


<span 
class="cmti-10x-x-109">oriented</span>.


There


is


always


a


very


explicit


goal


in


mind


in


a


well-posed


RL


problem,


and


Sutton


and


Barto


provide


the


following


examples:





     <ul class="itemize1">





     <li class="itemize">


     A


     master


     chess


     player


     deciding


     a


     move


     to


     make


     (goal:


     win


     the


     game)


     </li>





     <li class="itemize">


     An


     adaptive


     controller


     changing


     paramters


     of


     a


     businesses


     machines


     in


     real-time


     (goal:


     optimize


     output)


     </li>





     <li class="itemize">


     A


     gazelle


     struggling


     to


     walk


     after


     being


     born,


     but


     running


     20


     minutes


     later


     (goal:


     locomotion)


     </li>





     <li class="itemize">


     A


     mobile


     robot


     deciding


     to


     clean


     a


     new


     room


     or


     head


     back


     to


     it’s


     charging


     station


     to


     refuel


     (goal:


     minimize


     cleaning


     time)</li></ul>





<!--l. 77--><p class="noindent" >


<h3 class="likesectionHead"><a 
 id="x1-2000"></a><span id="textcolor3">Elements


of


RL</span></h3>


<div class="newtheorem">





<!--l. 79--><p class="noindent" ><span class="head">


<a 
 id="x1-2001r2"></a>








<span 
class="cmbx-10x-x-109">Definition 2.</span>


</span><span 
class="cmti-10x-x-109">The</span>


<span 
class="cmti-10x-x-109">policy</span>


defines


how


the


agent


behaves


in


a


given


state


or


time.


</div>


<div class="newtheorem">





<!--l. 83--><p class="noindent" ><span class="head">


<a 
 id="x1-2002r3"></a>








<span 
class="cmbx-10x-x-109">Definition 3.</span>


</span><span 
class="cmti-10x-x-109">The</span>


<span 
class="cmti-10x-x-109">reward</span>


<span 
class="cmti-10x-x-109">function</span>


defines


the


<span 
class="cmti-10x-x-109">goal</span>


of


the


RL


problem


to


the


agent,


and


maps


each


state


of


the


environment


to


a


single


number/reward.


<span 
class="cmti-10x-x-109">It</span>


<span 
class="cmti-10x-x-109">cannot</span>


<span 
class="cmti-10x-x-109">be</span>


<span 
class="cmti-10x-x-109">altered</span>


<span 
class="cmti-10x-x-109">by</span>


<span 
class="cmti-10x-x-109">the</span>


<span 
class="cmti-10x-x-109">agent,</span>


<span 
class="cmti-10x-x-109">but</span>


<span 
class="cmti-10x-x-109">it</span>


<span 
class="cmti-10x-x-109">can</span>


<span 
class="cmti-10x-x-109">alter</span>


<span 
class="cmti-10x-x-109">the</span>


<span 
class="cmti-10x-x-109">agent’s</span>


<span 
class="cmti-10x-x-109">policy</span>.


</div>


<div class="newtheorem">





<!--l. 87--><p class="noindent" ><span class="head">


<a 
 id="x1-2003r4"></a>








<span 
class="cmbx-10x-x-109">Definition 4.</span>


</span><span 
class="cmti-10x-x-109">The</span>


<span 
class="cmti-10x-x-109">value</span>


<span 
class="cmti-10x-x-109">function</span>


specifies


the


<span 
class="cmti-10x-x-109">long-term</span>


<span 
class="cmti-10x-x-109">value</span>


of


an


action/state.


The


value


of


the


state


is


the


total


amount


of


reward


the


agent


can


expect


to


accumulate


over


the


future,


starting


from


that


state.


</div>





<!--l. 91--><p class="noindent" >We


might


consider


rewards


to


be


a


more


<span 
class="cmti-10x-x-109">fundamental</span>


than


value,


because


without


reward,


there


is


no


value.


However,


in


RL


algorithms


(and


the


users,


us)


focus


much


more


on


value.


In


fact,


according


to


Sutton


and


Barto,


the


most


important


component


of


RL


problems


is


how


value


is


defined


and


computed.


It


is


a


much


more


difficult


factor


to


derive/compute,


because


it


must


be


estimated


and


re-estimated


based


on


the


everchanging


experience


of


the


agent.


<!--l. 93--><p class="noindent" >Some


problems


don’t


focus


on


value


at


all,


and


instead


focus


on


exploring


the


<span 
class="cmti-10x-x-109">space</span>


<span 
class="cmti-10x-x-109">of</span>


<span 
class="cmti-10x-x-109">policies</span>.


These


algorithms


are


called


<span 
class="cmti-10x-x-109">evolutionary</span>


<span 
class="cmti-10x-x-109">methods</span>


and


can


be


effective


on


certain


classes


of


problems.





<!--l. 95--><p class="noindent" >


<h3 class="likesectionHead"><a 
 id="x1-3000"></a><span id="textcolor4">Example:


Tic-Tac-Toe</span></h3>





<!--l. 97--><p class="noindent" >Now


we


discuss


how


a


RL


solution


might


look


for


an


agent


who


wants


to


win


tic-tac-toe.


The


goal


here


is


very


clear:


<!--l. 99--><p class="noindent" ><span 
class="cmbx-10x-x-109">Goal</span>:


Win


the


game.


<!--l. 101--><p class="noindent" >The


<span 
class="cmti-10x-x-109">policy</span>


would


be


the


rule


that


tells


the


agent/player


what


move


to


make


next


in


every


state


of


the


game.


<!--l. 103--><p class="noindent" >The


first


thing


we


want


to


do


is


have


a


table


of


scalars


bijectively


associated


associated


with


every


state


of


the


game.


This


scalar


will


represent


the


probability


that


we


will


win


the


game


given


that


state.


Hence,


this


is


our


<span 
class="cmti-10x-x-109">value</span>


<span 
class="cmti-10x-x-109">function</span>


to


be


learned


(we


will


update


this


function


as


we


play).


To


initialize


this


function


any


states


with


three


<span 
class="cmmi-10x-x-109">x</span>’s


in


a


row


will


be


assigned


1<span 
class="cmmi-10x-x-109">.</span>0


(since


we


already


won),


andy


state


with


three


<span 
class="cmmi-10x-x-109">o</span>’s


in


a


row


is


assigned


0<span 
class="cmmi-10x-x-109">.</span>0


(we


already


lost),


and


all


other


states


are


assigned


<img 
src="reinforcement_learning_chapter_one0x.png" alt=""  class="frac" align="middle">,


a


coin-tosses


chance.


<!--l. 105--><p class="noindent" >So


how


will


we


update


the


value


function?


It’s


intuitive


that


as


we


play


the


game


against


the


opponents


and


accumulate


sequences


of


boards,


we


want


the


<span 
class="cmti-10x-x-109">values</span>


<span 
class="cmti-10x-x-109">of</span>


<span 
class="cmti-10x-x-109">earlier</span>


<span 
class="cmti-10x-x-109">states</span>


<span 
class="cmti-10x-x-109">to</span>


<span 
class="cmti-10x-x-109">become</span>


<span 
class="cmti-10x-x-109">closer</span>


<span 
class="cmti-10x-x-109">to</span>


<span 
class="cmti-10x-x-109">the</span>


<span 
class="cmti-10x-x-109">values</span>


<span 
class="cmti-10x-x-109">of</span>


<span 
class="cmti-10x-x-109">the</span>


<span 
class="cmti-10x-x-109">later</span>


<span 
class="cmti-10x-x-109">states</span>.


<!--l. 107--><p class="noindent" >Let


<span 
class="cmmi-10x-x-109">E</span>


denote


the


set


of


possible


states/boards


(hence


<span 
class="cmsy-10x-x-109">|</span><span 
class="cmmi-10x-x-109">E</span><span 
class="cmsy-10x-x-109">| </span>=


19683


if


we


ignore


impossible


boards).


Then


let


<span 
class="cmmi-10x-x-109">V</span>


represent


the


value


function


such


that


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_one1x.png" alt="


" class="math-display" ></center>





<!--l. 110--><p class="nopar" >


Then


for


a


sequence


of


boards


representing


a


single


game


(in


which


it


was


the


<span 
class="cmti-10x-x-109">agent’s</span>


<span 
class="cmti-10x-x-109">turn</span>


<span 
class="cmti-10x-x-109">to</span>


<span 
class="cmti-10x-x-109">player</span>,


namely


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_one2x.png" alt="


" class="math-display" ></center>





<!--l. 114--><p class="nopar" >


we


udpate


the


<span 
class="cmmi-10x-x-109">V</span>


according


to


the


following


rule:


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_one3x.png" alt="


" class="math-display" ></center>





<!--l. 118--><p class="nopar" >


<!--l. 120--><p class="noindent" >Colloquially,


we


move


the


value


fo


the


board


an


<span 
class="cmmi-10x-x-109">α</span>


amount


in


the


value


of


the


next


boards


direction.


We


call


<span 
class="cmmi-10x-x-109">α</span>


the


<span 
class="cmti-10x-x-109">step</span>


<span 
class="cmti-10x-x-109">size</span>,


which


can


be


tuned


by


the


user.


Note


that


<span 
class="cmmi-10x-x-109">k &#x003C;</span>


<span 
class="cmmi-10x-x-109">N</span>,


and


this


recursive


definition


is


well


defined,


because


<span 
class="cmmi-10x-x-109">k </span>=


<span 
class="cmmi-10x-x-109">N</span>


falls


into


the


aforementioned


edge


cases.


 
</body> 
</html>














