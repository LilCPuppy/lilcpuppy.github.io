<!DOCTYPE html> 
<html lang="en-US" xml:lang="en-US" > 
<head>





<title></title> 
<meta  charset="utf-8" /> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" /> 
<link rel="stylesheet" type="text/css" href="reinforcement_learning_chapter_two.css" /> 
<meta name="src" content="reinforcement_learning_chapter_two.tex"> 
</head><body 
>





<!--l. 34--><p class="noindent" > 


 


<div class="center" 
>


<!--l. 37--><p class="noindent" >











<!--l. 40--><p class="noindent" ><span id="textcolor1"><span 
class="cmbx-12x-x-207">Chapter</span>


<span 
class="cmbx-12x-x-207">Two</span></span>





</div>


<!--l. 45--><p class="noindent" > 


 





<h3 class="likesectionHead"><a 
 id="x1-1000"></a><span id="textcolor2">Introduction</span></h3>





<!--l. 50--><p class="noindent" >The


most


important


feature


about


reinforcement


learning


from


other


types


of


the


learning


is


that


it


uses


training


data


to


<span 
class="cmti-10x-x-109">evaluate</span>


actions


rather


than


<span 
class="cmti-10x-x-109">instructing</span>


the


agent


on


what


to


do


with


correct


actions.


This


creates


the


need


for


exploration,


since


”purely


evaluative


feedback


indicates


only


how


good


the


action


is,


but


not


whether


it’s


the


best


or


worst


action


available”.


Another


key


distinction


between


evaluative


vs.


instructive


feedback


is


as


follows:





     <ul class="itemize1">





     <li class="itemize">


     Evaluative


     feedback


     depends


     entirely


     on


     the


     action


     taken,


     whereas


     </li>





     <li class="itemize">


     Instructive


     feedback


     is


     <span 
class="cmti-10x-x-109">independent</span>


     of


     the


     action


     taken


     (think


     making


     an


     arbitrary


     chess


     move,


     then


     being


     told


     which


     is


     actually


     the


     <span 
class="cmti-10x-x-109">right</span>


     one).


     In


     a


     sense,


     this


     is


     hardly


     feedback


     at


     all...</li></ul>





<!--l. 57--><p class="noindent" >


<h3 class="likesectionHead"><a 
 id="x1-2000"></a><span id="textcolor3">The


N-Armed


Bandit</span></h3>





<!--l. 59--><p class="noindent" >The


motivating


example


used


by


Sutton


and


Barto


is


the


n-armed


bandit.


Suppose


you


are


faced


with


a


set


of


<span 
class="cmmi-10x-x-109">n</span>


different


actions


or


choices


<table 
class="equation"><tr><td>





<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two0x.png" alt="


" class="math-display" ><a 
 id="x1-2001r1"></a></center></td><td class="equation-label">(1)</td></tr></table>





<!--l. 62--><p class="nopar" >








<!--l. 64--><p class="noindent" >and


when


picking


an


action,


you


recieve


a


reward


signal


chosen


from


a


statiionary


probability


distribution


associated


with


that


action,


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two1x.png" alt="


" class="math-display" ></center>





<!--l. 67--><p class="nopar" >


<!--l. 69--><p class="noindent" ><span 
class="cmbx-10x-x-109">Goal</span>:


Maximize


the


reward


signal


over


some


discrete


time.


<!--l. 71--><p class="noindent" >An


analogy


to


the


n-armed


bandit


would


be


the


n-armed


slot


machine,


where


each


lever


ahs


some


stationary


probability


distribution


giving


you


some


reward.


You


want


to


play


the


game


to


an


extent


where


you


can


determine


which


are


the


<span 
class="cmti-10x-x-109">good</span>


levers


(i.e.


associated


with


a


probability


distribution


with


a


high


expected


value),


and


play


exlusively


with


these


levers.


<!--l. 73--><p class="noindent" >As


we


just


mentioned,


each


lever


(or


arm/action)


has


an


associated


probability


distribution,


which


has


it’s


own


expected


value


and


variance.


This


motivates


a


definition.


<div class="newtheorem">





<!--l. 75--><p class="noindent" ><span class="head">


<a 
 id="x1-2002r1"></a>








<span 
class="cmbx-10x-x-109">Definition 1.</span>


</span>The


<span 
class="cmti-10x-x-109">value</span>


of


a


action


<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub><span 
class="cmmi-10x-x-109">, k </span>=


1<span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">…</span><span 
class="cmmi-10x-x-109">,n</span>,


is


the


<span 
class="cmti-10x-x-109">expected</span>


<span 
class="cmti-10x-x-109">value</span>


of


<span 
class="cmmi-10x-x-109">P</span><sub><span 
class="cmmi-8">a</span><sub><span 
class="cmmi-6">k</span></sub></sub>,


denoted


<span 
class="cmmi-10x-x-109">Q</span><sup><span 
class="cmsy-8">∗</span></sup>(<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub>).


i.e.


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two2x.png" alt="


" class="math-display" ></center>





<!--l. 79--><p class="nopar" >


</div>


 
</body> 
</html>














