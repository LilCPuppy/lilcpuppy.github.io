<!DOCTYPE html> 
<html lang="en-US" xml:lang="en-US" > 
<head>





<title></title> 
<meta  charset="utf-8" /> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" /> 
<link rel="stylesheet" type="text/css" href="reinforcement_learning_chapter_two.css" /> 
<meta name="src" content="reinforcement_learning_chapter_two.tex"> 
</head><body 
>





<!--l. 34--><p class="noindent" >¬†


¬†


<div class="center" 
>


<!--l. 37--><p class="noindent" >











<!--l. 40--><p class="noindent" ><span id="textcolor1"><span 
class="cmbx-12x-x-207">Chapter</span>


<span 
class="cmbx-12x-x-207">Two</span></span>





</div>


<!--l. 45--><p class="noindent" >¬†


¬†





<h3 class="likesectionHead"><a 
 id="x1-1000"></a><span id="textcolor2">Introduction</span></h3>





<!--l. 50--><p class="noindent" >The


most


important


feature


about


reinforcement


learning


from


other


types


of


the


learning


is


that


it


uses


training


data


to


<span 
class="cmti-10x-x-109">evaluate</span>


actions


rather


than


<span 
class="cmti-10x-x-109">instructing</span>


the


agent


on


what


to


do


with


correct


actions.


This


creates


the


need


for


exploration,


since


‚Äùpurely


evaluative


feedback


indicates


only


how


good


the


action


is,


but


not


whether


it‚Äôs


the


best


or


worst


action


available‚Äù.


Another


key


distinction


between


evaluative


vs.


instructive


feedback


is


as


follows:





     <ul class="itemize1">





     <li class="itemize">


     Evaluative


     feedback


     depends


     entirely


     on


     the


     action


     taken,


     whereas


     </li>





     <li class="itemize">


     Instructive


     feedback


     is


     <span 
class="cmti-10x-x-109">independent</span>


     of


     the


     action


     taken


     (think


     making


     an


     arbitrary


     chess


     move,


     then


     being


     told


     which


     is


     actually


     the


     <span 
class="cmti-10x-x-109">right</span>


     one).


     In


     a


     sense,


     this


     is


     hardly


     feedback


     at


     all...</li></ul>





<!--l. 57--><p class="noindent" >


<h3 class="likesectionHead"><a 
 id="x1-2000"></a><span id="textcolor3">The


N-Armed


Bandit</span></h3>





<!--l. 59--><p class="noindent" >The


motivating


example


used


by


Sutton


and


Barto


is


the


n-armed


bandit.


Suppose


you


are


faced


with


a


set


of


<span 
class="cmmi-10x-x-109">n</span>


different


actions


or


choices


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two0x.png" alt="


" class="math-display" ></center>





<!--l. 62--><p class="nopar" >


<!--l. 64--><p class="noindent" >and


when


picking


an


action,


you


recieve


a


reward


signal


chosen


from


a


statiionary


probability


distribution


associated


with


that


action,


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two1x.png" alt="


" class="math-display" ></center>





<!--l. 67--><p class="nopar" >


<!--l. 69--><p class="noindent" ><span 
class="cmbx-10x-x-109">Goal</span>:


Maximize


the


reward


signal


over


some


discrete


time.


<!--l. 71--><p class="noindent" >An


analogy


to


the


n-armed


bandit


would


be


the


n-armed


slot


machine,


where


each


lever


ahs


some


stationary


probability


distribution


giving


you


some


reward.


You


want


to


play


the


game


to


an


extent


where


you


can


determine


which


are


the


<span 
class="cmti-10x-x-109">good</span>


levers


(i.e.


associated


with


a


probability


distribution


with


a


high


expected


value),


and


play


exlusively


with


these


levers.


<!--l. 73--><p class="noindent" >As


we


just


mentioned,


each


lever


(or


arm/action)


has


an


associated


probability


distribution,


which


has


it‚Äôs


own


expected


value


and


variance.


This


motivates


a


definition.


<div class="newtheorem">





<!--l. 75--><p class="noindent" ><span class="head">


<a 
 id="x1-2001r1"></a>








<span 
class="cmbx-10x-x-109">Definition 1.</span>


</span>The


<span 
class="cmti-10x-x-109">value</span>


of


a


action


<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub><span 
class="cmmi-10x-x-109">,</span><span style="margin-left:3.04076pt" class="tmspace"></span><span 
class="cmmi-10x-x-109">k </span>=


1<span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">‚Ä¶</span><span 
class="cmmi-10x-x-109">,n</span>,


is


the


<span 
class="cmti-10x-x-109">expected</span>


<span 
class="cmti-10x-x-109">value</span>


of


<span 
class="cmmi-10x-x-109">P</span><sub><span 
class="cmmi-8">a</span><sub><span 
class="cmmi-6">k</span></sub></sub>,


denoted


<span 
class="cmmi-10x-x-109">Q</span><sup><span 
class="cmsy-8">‚àó</span></sup>(<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub>).


i.e.


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two2x.png" alt="


" class="math-display" ></center>





<!--l. 79--><p class="nopar" >


</div>


<!--l. 82--><p class="noindent" >Obviously,


if


we


knew


the


expected


values


of


all


the


distributions


beforehand,


there


would


be


no


issue,


because


we


would


pick


the


action


with


the


highest


value.


However,


since


we


don‚Äôt,


we


can


only


attain


estimates


of


the


values


based


on


a


historical


record


or


picking


an


action.


Hence,


we


have


a


tradeoff:


do


we


exploit


the


levers


we


know?


Or


do


we


explore


and


improve


teh


estimate


of


the


levers


that


we


don‚Äôt?


Over


a


period


of


time,


attaining


the


maximum


reward


may


require


a


combination


of


these.


<!--l. 84--><p class="noindent" >This


motivates


the


question,


how


should


we


estimate


<span 
class="cmmi-10x-x-109">Q</span><sup><span 
class="cmsy-8">‚àó</span></sup>(<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub>)?


If


at


time


<span 
class="cmmi-10x-x-109">t</span>


we


have


chosen


an


action


<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub>


a


total


<span 
class="cmmi-10x-x-109">m</span><sub><span 
class="cmmi-8">k</span></sub>


times,


then


it‚Äôs


intuitive


to


estimate


<span 
class="cmmi-10x-x-109">Q</span><sup><span 
class="cmsy-8">‚àó</span></sup>(<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub>)


as:


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two3x.png" alt="


" class="math-display" ></center>





<!--l. 87--><p class="nopar" >


<!--l. 89--><p class="noindent" >where


we


define


<span 
class="cmmi-10x-x-109">Q</span><sub><span 
class="cmr-8">0</span></sub>(<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub>) :=


0


(or


some


other


default


value),


and


<span 
class="cmmi-10x-x-109">r</span><sub><span 
class="cmmi-8">l</span></sub>


is


the


<span 
class="cmmi-10x-x-109">l</span><span 
class="cmsy-10x-x-109">‚àí</span>th


return


from


action


<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub>.


We


note


that


this


estimate


has


some


measure


of


‚Äùgoodness‚Äù


when


observing


that


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two4x.png" alt="


" class="math-display" ></center>





<!--l. 92--><p class="nopar" >


<!--l. 94--><p class="noindent" >Clearly,


we


don‚Äôt


want


to


play


greedily,


i.e.


always


pick


an


action


<span 
class="cmmi-10x-x-109">a</span><sup><span 
class="cmsy-8">‚àó</span></sup>


such


that


<span 
class="cmmi-10x-x-109">a</span><sup><span 
class="cmsy-8">‚àó</span></sup> =


arg max<sub><span 
class="cmmi-8">a</span></sub><span style="margin-left:3.04076pt" class="tmspace"></span><span 
class="cmmi-10x-x-109">Q</span><sub><span 
class="cmmi-8">t</span></sub>(<span 
class="cmmi-10x-x-109">a</span>)


since


we


would


never


explore


(beyond


some


action


set


we


already


know).


<!--l. 96--><p class="noindent" >A


way


to


promote


this


exploration


is


to


have


an


<span 
class="cmmi-10x-x-109">ùúñ</span>


chance


to


explore


(and


pick


<span 
class="cmmi-10x-x-109">a</span><sup><span 
class="cmsy-8">‚àó</span></sup>


randomly


from


the


unknown


actions),


and


<span 
class="cmmi-10x-x-109">a</span><sup><span 
class="cmsy-8">‚àó</span></sup> =


arg max<sub><span 
class="cmmi-8">a</span></sub><span style="margin-left:3.04076pt" class="tmspace"></span><span 
class="cmmi-10x-x-109">Q</span><sub><span 
class="cmmi-8">t</span></sub>(<span 
class="cmmi-10x-x-109">a</span>)


the


rest


of


the


time.


These


methods


are


called


<span 
class="cmmi-10x-x-109">ùúñ</span><span 
class="cmsy-10x-x-109">‚àí</span>greedy


methods.


In


the


asymptotic


case,


we


see


that


we


will


ahave


to


eventually


pick


every


action


an


infinite


number


of


times,


and


hence


we


maintain


that


under


this


<span 
class="cmmi-10x-x-109">ùúñ</span><span 
class="cmsy-10x-x-109">‚àí</span>greedy


policy,


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two5x.png" alt="


" class="math-display" ></center>





<!--l. 99--><p class="nopar" >





<!--l. 101--><p class="noindent" >


<h3 class="likesectionHead"><a 
 id="x1-3000"></a><span id="textcolor4">Softmax


Action


Selection</span></h3>





<!--l. 103--><p class="noindent" >The


last


segment


detailed


a


method


for


our


agent


to


choose


an


<span 
class="cmti-10x-x-109">action</span>.


Another


way


to


pick


an


action


is


to


assign


each


choice


a


probability.


For


example,


say


we


pick


<span 
class="cmmi-10x-x-109">a</span>


with


probability


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two6x.png" alt="


" class="math-display" ></center>





<!--l. 106--><p class="nopar" >


<!--l. 108--><p class="noindent" >where


<span 
class="cmmi-10x-x-109">b</span>


iterates


over


the


possible


actions,


and


<span 
class="cmmi-10x-x-109">œÑ</span>


corresponds


to


the


<span 
class="cmti-10x-x-109">temperature</span>


picked


by


the


user.


High


temperatures


will


correspond


to


the


actions


being


close


to


equiprobable,


and


low


temperatures


cuase


a


greater


difference


in


selection


probability.


In


fact,


as


<span 
class="cmmi-10x-x-109">œÑ </span><span 
class="cmsy-10x-x-109">‚Üí</span>


0,


the


softmax


action


selection


becomes


the


same


as


the


greedy


action


selection.


<!--l. 110--><p class="noindent" >It‚Äôs


unclear


whether


<span 
class="cmmi-10x-x-109">ùúñ</span><span 
class="cmsy-10x-x-109">‚àí</span>greedy


action


selection


or


softmax


action


selection


is


more


effective.


The


<span 
class="cmmi-10x-x-109">ùúñ</span>


methods


are


easier


to


tune


and


dynamically


change,


whereas


the


softmax


selection


means


picking


<span 
class="cmmi-10x-x-109">œÑ</span>,


hence


knowing


more


about


the


nature


of


the


distributions


specific


to


the


actions.





<!--l. 112--><p class="noindent" >


<h3 class="likesectionHead"><a 
 id="x1-4000"></a><span id="textcolor5">Evaluation


vs.


Instruction</span></h3>





<!--l. 114--><p class="noindent" >The


n-armed


bandit


problem


gives


us


an


example


of


purely


evaluative


feedback.


In


reinforcement


learning,


the


‚Äùfeedback‚Äù


is


independent


of


the


action.


So


it‚Äôs


not


‚Äùreal‚Äù


feedback.


Supervised


learning


algorithms


try


to


configure/shape


themselves


to


their


environment


-


but


they


cannot


learn


to


control


or


influence


their


environments.





<!--l. 116--><p class="noindent" >


<h4 class="likesubsectionHead"><a 
 id="x1-5000"></a><span id="textcolor6">Example</span></h4>





<!--l. 118--><p class="noindent" >Let‚Äôs


dig


into


these


differences


by


emphasizing


an


example


given


by


Sutton


and


Barto.


Consider


the


n-armed


bandit


problem


with


2


arms


(i.e.


<span 
class="cmmi-10x-x-109">n </span>=


10).


Each


action


has


two


rewards,


success


and


failure


(binary


actions).


In


a


supervised


learning


setting


we


might


infer


an


action


was


‚Äùcorrect‚Äù


if


you


got


success,


and


if


you


got


failure,


you


may


assume


the


other


action


was


‚Äùsuccess‚Äù.


Then


as


you


play,


you


can


tally


how


many


times


an


action


was


inferred


to


be


correct,


and


you


pick


the


action


based


on


the


tallys


(more


tallys


=


higher


chance


of


winning).


<!--l. 120--><p class="noindent" >Suppose


the


actions


we‚Äôre


deterministic


(i.e.


they‚Äôre


assigned


be


either


completely


success


or


completely


failure),


then


this


algorithm


would


never


be


wrong.


After


a


single


choice,


we


would


know


exactly


what


action


we


would


fix


on.


But


if


the


actions


were


stochastic


(each


action


has


a


probability


of


success),


then


this


is


certainly


not


the


case.


<!--l. 122--><p class="noindent" >In


this


case,


the


supervised


learning


approach


would


work


only


if


the


probabilities


fulfill


the


assumption


that


one


action


is


bad


(failure)


and


the


other


is


good


(success).


Hence,


we


need


one


action


will


probability


of


success


greater


than


<img 
src="reinforcement_learning_chapter_two7x.png" alt=""  class="frac" align="middle">,


with


the


other


having


a


probability


of


success


less


than


<img 
src="reinforcement_learning_chapter_two8x.png" alt=""  class="frac" align="middle">.


Then,


the


algorithm


would


fixate


on


the


better


choice,


and


we


would


win.


<!--l. 124--><p class="noindent" >But


if


both


were


good


(probability


of


success


over


<img 
src="reinforcement_learning_chapter_two9x.png" alt=""  class="frac" align="middle">),


or


both


were


bad


(probability


of


success


less


than


<img 
src="reinforcement_learning_chapter_two10x.png" alt=""  class="frac" align="middle">),


than


the


supervised


learning


methods


how


no


way


of


determining


which


choice


is


better


(or


worse).


In


the


both


bad


case,


the


algorithm


would


likely


pick


a


failing


action,


and


always


assume


the


other


action


is


better.


When


it


picks


the


other


option


(and


also


gets


failure),


it


would


flip


back


to


the


first


choice.


This


cycles,


and


we


get


an


oscillating


algorithm


that


cannot


truly


find


the


better


choice


of


the


two.


In


the


both


good


case,


we


would


quickly


fixate


on


a


single


action


(since


it


usually


wins,


the


other


must


usually


lose,


right?),


typically,


the


first


action


picked.


<!--l. 126--><p class="noindent" >So


what


does


a


reinforcement


learning


approach


look


like?


Let


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two11x.png" alt="


" class="math-display" ></center>





<!--l. 129--><p class="nopar" >


<!--l. 131--><p class="noindent" >We


then


define


the


next


timestep


as


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two12x.png" alt="


" class="math-display" ></center>





<!--l. 134--><p class="nopar" >


<!--l. 136--><p class="noindent" >If


the


action


inferred


to


be


correct


on


play


<span 
class="cmmi-10x-x-109">t</span>


was


<span 
class="cmmi-10x-x-109">d</span><sub><span 
class="cmmi-8">t</span></sub>


,


then


we


update


<span 
class="cmmi-10x-x-109">œÄ</span><sub><span 
class="cmmi-8">t</span></sub>(<span 
class="cmmi-10x-x-109">d</span><sub><span 
class="cmmi-8">t</span></sub>)


an


<span 
class="cmmi-10x-x-109">Œ±</span>


amount


from


it‚Äôs


current


value


toward


1


We


then


inversely


adjust


the


other


probabilities


so


they


sum


to


one.


We


call


this


algorithms


the


<span 
class="cmmi-10x-x-109">L</span><sub><span 
class="cmmi-8">R</span><span 
class="cmsy-8">‚àí</span><span 
class="cmmi-8">P</span> </sub>


(linear,


reward-penalty)


algorithm.


If


we


do


nothing


on


a


failure,


we


call


this


the


<span 
class="cmmi-10x-x-109">L</span><sub><span 
class="cmmi-8">R</span><span 
class="cmsy-8">‚àí</span><span 
class="cmmi-8">I</span></sub>


algorithm


(linear,


reward-inaction).


<!--l. 138--><p class="noindent" >We


note


that


these


essentially


take


supervised


learning


methods,


and


make


them


stochastic.





<!--l. 140--><p class="noindent" >


<h3 class="likesectionHead"><a 
 id="x1-6000"></a><span id="textcolor7">Incremental


Implementation</span></h3>





<!--l. 142--><p class="noindent" >Recall


that


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two13x.png" alt="


" class="math-display" ></center>





<!--l. 145--><p class="nopar" >


and


notice


that


we


can


compute


this


incrementally:


<div class="align"><img 
src="reinforcement_learning_chapter_two14x.png" alt="pict" ></div>


<!--l. 155--><p class="noindent" >Hence,


in


order


to


compute


<span 
class="cmmi-10x-x-109">Q</span><sub><span 
class="cmmi-8">k</span><span 
class="cmr-8">+1</span></sub>,


we


only


need


to


know


<span 
class="cmmi-10x-x-109">Q</span><sub><span 
class="cmmi-8">k</span></sub>


and


<span 
class="cmmi-10x-x-109">k</span>.


Moreover,


we


can


see


how


this


is


adjusting


<span 
class="cmmi-10x-x-109">Q</span><sub><span 
class="cmmi-8">k</span><span 
class="cmr-8">+1</span></sub>


with


regards


to


the


old


estimate;


we


are


moving


<span 
class="cmmi-10x-x-109">Q</span><sub><span 
class="cmmi-8">k</span></sub>


a


<img 
src="reinforcement_learning_chapter_two15x.png" alt=""  class="frac" align="middle">


amount


(the


stepsize!)


from


the


old


value,


<span 
class="cmmi-10x-x-109">Q</span><sub><span 
class="cmmi-8">k</span></sub>,


to


the


latest


estimate,


<span 
class="cmmi-10x-x-109">r</span><sub><span 
class="cmmi-8">k</span><span 
class="cmr-8">+1</span></sub>.





<!--l. 157--><p class="noindent" >


<h4 class="likesubsectionHead"><a 
 id="x1-7000"></a><span id="textcolor8">Tracking


a


Non-Stationary


Problem</span></h4>





<!--l. 159--><p class="noindent" >Up


until


now,


we


have


been


managing


a


stationary


problem,


i.e.


<span 
class="cmti-10x-x-109">the</span>


<span 
class="cmti-10x-x-109">bandit</span>


<span 
class="cmti-10x-x-109">it</span>


<span 
class="cmti-10x-x-109">not</span>


<span 
class="cmti-10x-x-109">changing</span>.


However,


what


if


the


bandit


does


decide


to


change


over


time?


<!--l. 161--><p class="noindent" >In


this


case,


we


want


to


weigh


the


most


recent


rewards


over


the


old


ones.


One


of


the


popular


ways


to


do


this


is


to


use


a


constant


stepsize


parameter.


<div class="align"><img 
src="reinforcement_learning_chapter_two16x.png" alt="pict" ></div>


<!--l. 170--><p class="noindent" >By


noticing


that


(1<span 
class="cmsy-10x-x-109">‚àí</span>


<span 
class="cmmi-10x-x-109">Œ±</span>)<sup><span 
class="cmmi-8">k</span></sup>+


<span 
class="cmex-10x-x-109">‚àë</span>
   <sub><span 
class="cmmi-8">l</span></sub><span 
class="cmmi-10x-x-109">Œ±</span>(1<span 
class="cmsy-10x-x-109">‚àí</span>


<span 
class="cmmi-10x-x-109">Œ±</span>)<sup><span 
class="cmmi-8">k</span><span 
class="cmsy-8">‚àí</span><span 
class="cmmi-8">l</span></sup> =


1,


we


see


that


this


is


in


fact


a


convex


combination


of


<span 
class="cmmi-10x-x-109">Q</span><sub><span 
class="cmr-8">0</span></sub><span 
class="cmmi-10x-x-109">,r</span><sub><span 
class="cmr-8">1</span></sub><span 
class="cmmi-10x-x-109">,</span><span 
class="cmmi-10x-x-109">‚Ä¶</span><span 
class="cmmi-10x-x-109">,r</span><sub><span 
class="cmmi-8">k</span><span 
class="cmr-8">+1</span></sub>.


Moreover,


we


see


that


the


coefficients


of


the


older


observations


are


following


an


exponential


decay


according


to


the


exponent


of


(1<span 
class="cmsy-10x-x-109">‚àí</span>


<span 
class="cmmi-10x-x-109">Œ±</span>).


As


a


result,


this


method


is


called,


<span 
class="cmti-10x-x-109">exponential,</span>


<span 
class="cmti-10x-x-109">recency</span>


<span 
class="cmti-10x-x-109">weighted</span>


<span 
class="cmti-10x-x-109">average</span>.


<!--l. 172--><p class="noindent" >What


if


we


wanted


to


generalize


further?


Let


<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub>(<span 
class="cmmi-10x-x-109">a</span>)


denote


the


stepsize


paramter,


then


to


guarantee


that


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two17x.png" alt="


" class="math-display" ></center>





<!--l. 175--><p class="nopar" >


<!--l. 177--><p class="noindent" >then


we


need


the


conditions


that


<span 
class="cmex-10x-x-109">‚àë</span>
  <sub><span 
class="cmmi-8">k</span><span 
class="cmr-8">=1</span></sub><sup><span 
class="cmsy-8">‚àû</span></sup><span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub>(<span 
class="cmmi-10x-x-109">a</span>) =


<span 
class="cmsy-10x-x-109">‚àû</span>,


and,


<span 
class="cmex-10x-x-109">‚àë</span>
  <sub><span 
class="cmmi-8">k</span><span 
class="cmr-8">=1</span></sub><sup><span 
class="cmsy-8">‚àû</span></sup><span class="big"><img 
src="reinforcement_learning_chapter_two18x.png" alt=""  class="left" align="middle"></span><span 
class="cmmi-10x-x-109">Œ±</span><sub><span 
class="cmmi-8">k</span></sub>(<span 
class="cmmi-10x-x-109">a</span>)<span class="big"><img 
src="reinforcement_learning_chapter_two19x.png" alt=""  class="left" align="middle"></span><sup><span 
class="cmr-8">2</span></sup> <span 
class="cmmi-10x-x-109">&#x003C;</span>


<span 
class="cmsy-10x-x-109">‚àû</span>.


This


ensures


that


the


steps


are


<span 
class="cmti-10x-x-109">large</span>


enough


to


overcome


the


initial


noise/uncertainty


of


the


problem,


and


that


they


are


<span 
class="cmti-10x-x-109">small</span>


enough


to


assure


convergence.


<!--l. 179--><p class="noindent" >We


notice


that


for


the


average


sampling


method,


where


<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub> =


<img 
src="reinforcement_learning_chapter_two20x.png" alt=""  class="frac" align="middle">,


these


conditions


are


met


(harmonic


series


diverges,


and


p-series


(<span 
class="cmmi-10x-x-109">p </span>=


2)


converges)!


But


this


is


<span 
class="cmti-10x-x-109">not</span>


the


case


for


<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">k</span></sub> =


<span 
class="cmmi-10x-x-109">Œ±</span>


(a


constant).


Hence,


our


non-stationary


method


will


not


converge,


which


is


fine!


It‚Äôs


a


non-stationary


problem!


The


term


<span 
class="cmti-10x-x-109">convergence</span>


doesn‚Äôt


make


sense


in


this


case.





<!--l. 181--><p class="noindent" >


<h3 class="likesectionHead"><a 
 id="x1-8000"></a><span id="textcolor9">Optimistic


Initial


Values</span></h3>





<!--l. 183--><p class="noindent" >All


the


methods


so


far


have


been


dependent


on


some


initial


action-value


estimates


(<span 
class="cmmi-10x-x-109">Q</span><sub><span 
class="cmr-8">0</span></sub>).


So


they


will


all


be


biased


by


initial


estimates


to


some


degree.


Note


that


for


sample


averaging


methods,


the


bias


completely


disappears


once


all


the


actions


have


been


chosen


at


least


once


(so


we


can


essentially


ignore


it),


but


this


is


not


the


case


for


the


constant


<span 
class="cmmi-10x-x-109">Œ±</span><sub><span 
class="cmmi-8">k</span></sub> =


<span 
class="cmmi-10x-x-109">Œ±</span>.


<!--l. 185--><p class="noindent" >We


leave


the


details


to


the


book,


but


this


need


not


be


a


bad


thing!


The


biased


initial


estimates


can


be


used


to


promote


exploration


in


our


agent.


See


the


book


for


more


:).





<!--l. 187--><p class="noindent" >


<h3 class="likesectionHead"><a 
 id="x1-9000"></a><span id="textcolor10">Reinforcement


Comparison</span></h3>





<!--l. 189--><p class="noindent" >A


central,


underlying


idea


in


RL


algorithms


is


that


actions


followed


by


large


positive


rewards


should


be


more


likely


to


occur,


and


actions


followed


by


small


rewards


(or


penalties!)


should


be


less


likley


to


occur.


<!--l. 191--><p class="noindent" >But


how


does


the


learner


know


what‚Äôs


good?


A


reference


reward


is


needed


for


this,


and


a


natural


choice


is


the


average


of


the


previously


chosen


rewards.


Algorithms


using


this


approach


are


called


<span 
class="cmti-10x-x-109">reinforcement</span>


<span 
class="cmti-10x-x-109">comparison</span>


<span 
class="cmti-10x-x-109">methods</span>,


and


can


be


sometimes


more


effective


than


action-value


methods.


<!--l. 193--><p class="noindent" >The


difference


between


the


two


(reinforcement


comparison


and


action-value)


is


that


reinforcement


comparison


methods


don‚Äôt


keep


track


of


the


individual


action


values,


instead


they


keep


a


holistic,


overall


reward.


To


pick


among


the


actions,


they


maintain


a


separate


measure


for


their


preference,


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two21x.png" alt="


" class="math-display" ></center>





<!--l. 196--><p class="nopar" >


which


is


used


to


calculate


the


probability


of


picking


action


<span 
class="cmmi-10x-x-109">a</span>.


<!--l. 199--><p class="noindent" >An


example


might


be


to


use


the


softmax


relationship:


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two22x.png" alt="


" class="math-display" ></center>





<!--l. 202--><p class="nopar" >


where


Œ†<sub><span 
class="cmmi-8">t</span></sub>(<span 
class="cmmi-10x-x-109">a</span>)


denotes


the


probability


of


picking


action


<span 
class="cmmi-10x-x-109">a</span>.


We


can


then


possibly


increment


the


<span 
class="cmmi-10x-x-109">p</span><sub><span 
class="cmmi-8">t</span></sub>‚Äôs


according


to


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two23x.png" alt="


" class="math-display" ></center>





<!--l. 206--><p class="nopar" >


where


<span 
class="cmmi-10x-x-109">r</span><sub><span 
class="cmmi-8">t</span></sub><sup><span 
class="cmsy-8">‚àó</span></sup>


is


the


reference


reward


(holistic


measure),


and


we


also


increment


<span 
class="cmmi-10x-x-109">r</span><sub><span 
class="cmmi-8">t</span><span 
class="cmr-8">+1</span></sub><sup><span 
class="cmsy-8">‚àó</span></sup> =


<span 
class="cmmi-10x-x-109">r</span><sub><span 
class="cmmi-8">t</span></sub><sup><span 
class="cmsy-8">‚àó</span></sup>+


<span 
class="cmmi-10x-x-109">Œ±</span>(<span 
class="cmmi-10x-x-109">r</span><sub><span 
class="cmmi-8">t</span></sub><span 
class="cmsy-10x-x-109">‚àí</span>


<span 
class="cmmi-10x-x-109">r</span><sub><span 
class="cmmi-8">t</span></sub><sup><span 
class="cmsy-8">‚àó</span></sup>).





<!--l. 209--><p class="noindent" >


<h3 class="likesectionHead"><a 
 id="x1-10000"></a><span id="textcolor11">Pursuint


Methods</span></h3>





<!--l. 211--><p class="noindent" >We


can


refer


to


the


last


section


as


using


<span 
class="cmti-10x-x-109">action</span>


<span 
class="cmti-10x-x-109">preferences</span>


rather


than


action


values.


Pursuint


methods


will


combine


both


of


these


approaches,


the


action


preferences


are


set


to


Œ†<sub><span 
class="cmmi-8">t</span></sub>(<span 
class="cmmi-10x-x-109">a</span>),


and


after


each


play


they


are


updated


to


amke


the


greedy


action


more


likely


to


be


selected.


After


the


<span 
class="cmmi-10x-x-109">t</span>-th


play,


let


<span 
class="cmmi-10x-x-109">a</span><sup><span 
class="cmsy-8">‚àó</span></sup> =


arg max<sub><span 
class="cmmi-8">a</span></sub><span style="margin-left:3.04076pt" class="tmspace"></span><span 
class="cmmi-10x-x-109">Q</span><sub><span 
class="cmmi-8">t</span></sub>(<span 
class="cmmi-10x-x-109">a</span>),


then


the


probability


of


selecting


<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">t</span><span 
class="cmr-8">+1</span></sub> =


<span 
class="cmmi-10x-x-109">a</span><sub><span 
class="cmmi-8">t</span><span 
class="cmr-8">+1</span></sub><sup><span 
class="cmsy-8">‚àó</span></sup>


is


incremented


a


<span 
class="cmmi-10x-x-109">Œ≤</span>


of


a


way


towards


1,


<center class="math-display" >





<img 
src="reinforcement_learning_chapter_two24x.png" alt="


" class="math-display" ></center>





<!--l. 214--><p class="nopar" >


while


the


rest


of


the


probabilities


are


evenly


decremented


towards


zero.





<!--l. 217--><p class="noindent" >


<h3 class="likesectionHead"><a 
 id="x1-11000"></a><span id="textcolor12">Associative


Search</span></h3>





<!--l. 219--><p class="noindent" >Suppose


that


there


are


mulptile


n-armed


bandits,


and


we


encounter


each


one


randomly


on


every


play.


If


we


have


no


indication


as


to


which


bandit


is


which,


we


might


be


toast


unless


the


true


action


values


among


the


bandits


were


very


similar


and


changed


slowly


with


respect


to


time.


But


what


is


there


was


an


indication?


Suppose


each


was


assigned


a


different


color,


then


our


agent


could


start


to


associate


rules


with


each


signal,


<div class="center" 
>


<!--l. 220--><p class="noindent" >











<!--l. 221--><p class="noindent" >‚Äùif


red,


do


this,


if


blue,


do


this,


...</div>


<!--l. 224--><p class="noindent" >This


is


now


an


associative


search,


because


it


involves


both


trial


and


error


to


find


the


best


actions,


but


also


needs


to


associate


the


best


actions


with


the


individual


bandits.


And


now


we


are


almost


at


a


full


RL


problem,


with


the


only


ingredients


left


being


to


let


our


actions


affect


the


next


situations.


 
</body> 
</html>














